{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b2505ac",
   "metadata": {},
   "source": [
    "# 1. Mô tả"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b2a8a",
   "metadata": {},
   "source": [
    "Ở notebook này chúng ta sẽ có hai bước chính:\n",
    "\n",
    "1. Tiền xử lý sơ bộ: Dựa vào insights rút ra từ notebook 1\n",
    "    - Loại bỏ 2 cột Naive bayes\n",
    "    - Encode các biến phân loại có thứ tự\n",
    "    - Giữ lại top 10 cột tương quan cao tìm thấy ở notebook 1\n",
    "\n",
    "2. Tạo giả thuyết và chuẩn bị tập train, valid, test cho bước modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa1ee2f",
   "metadata": {},
   "source": [
    "## Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a0177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import các thư viện cần thiết\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Cấu hình hiển thị\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3732d6c",
   "metadata": {},
   "source": [
    "## Khai báo hàm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf3a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# khai báo các hàm sẽ sử dụng (có thể bắt chước, copy lại hàm từ notebook1)\n",
    "def clean_string_columns(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Làm sạch các cột dạng chuỗi trong structured array.\n",
    "\n",
    "    Input:\n",
    "        data (np.ndarray): structured array đọc từ np.genfromtxt\n",
    "    Output:\n",
    "        np.ndarray: structured array sau khi đã strip khoảng trắng và dấu nháy (\", ')\n",
    "    Nhiệm vụ:\n",
    "        Do dữ liệu categorical đôi khi có dạng '\"Blue\"', '\"M\"', ... nên cần bỏ dấu nháy để xử lý đúng.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    for name in data.dtype.names:\n",
    "        kind = data.dtype[name].kind  # 'U' (unicode), 'S' (bytes), 'O' (object), ...\n",
    "        if kind in (\"U\", \"S\", \"O\"):\n",
    "            col = data[name]\n",
    "            if kind == \"S\":\n",
    "                col = np.char.decode(col, \"utf-8\", errors=\"ignore\")\n",
    "            data[name] = np.char.strip(col.astype(str), chars=\" \\\"'\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data_numpy(filepath: str, clean_strings: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Đọc dữ liệu CSV bằng NumPy (structured array).\n",
    "\n",
    "    Input:\n",
    "        filepath (str): đường dẫn tới file CSV\n",
    "        clean_strings (bool): nếu True thì làm sạch các cột dạng chuỗi sau khi load\n",
    "    Output:\n",
    "        np.ndarray: structured array chứa dữ liệu + tên cột (names=True)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Lỗi: Không tìm thấy file tại đường dẫn: {filepath}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        data = np.genfromtxt(filepath, delimiter=\",\", dtype=None, names=True, encoding=\"utf-8\")\n",
    "        if clean_strings:\n",
    "            data = clean_string_columns(data)\n",
    "        print(\"Đã load dữ liệu thành công!\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Có lỗi xảy ra khi đọc file: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def drop_columns_structured(data: np.ndarray, drop_cols: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Loại bỏ một danh sách cột khỏi structured array.\n",
    "\n",
    "    Input:\n",
    "        data (np.ndarray): structured array\n",
    "        drop_cols (list): danh sách tên cột cần loại\n",
    "    Output:\n",
    "        np.ndarray: structured array chỉ còn các cột giữ lại\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    cols = list(data.dtype.names)\n",
    "    keep_cols = [c for c in cols if c not in set(drop_cols)]\n",
    "    return data[keep_cols]\n",
    "\n",
    "\n",
    "def drop_naive_bayes_columns(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Loại bỏ 2 cột Naive_Bayes_Classifier_* (rác) khỏi dataset.\n",
    "\n",
    "    Input:\n",
    "        data (np.ndarray): structured array\n",
    "    Output:\n",
    "        np.ndarray: structured array đã loại các cột Naive_Bayes\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    cols = list(data.dtype.names)\n",
    "    drop_cols = [c for c in cols if \"Naive_Bayes_Classifier\" in c]\n",
    "\n",
    "    if len(drop_cols) == 0:\n",
    "        print(\"Không thấy cột Naive_Bayes_Classifier nào để loại.\")\n",
    "        return data\n",
    "\n",
    "    print(\"Các cột bị loại:\")\n",
    "    for c in drop_cols:\n",
    "        print(\"-\", c)\n",
    "\n",
    "    return drop_columns_structured(data, drop_cols)\n",
    "\n",
    "\n",
    "def encode_ordinal(col: np.ndarray, ordered_values: list, unknown_value: float = np.nan) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Mã hoá một cột categorical theo thứ tự (ordinal encoding).\n",
    "\n",
    "    Input:\n",
    "        col (np.ndarray): mảng 1 chiều của cột categorical\n",
    "        ordered_values (list): danh sách mức theo thứ tự (mức 0 -> mức 1 -> ...)\n",
    "        unknown_value (float): giá trị gán cho các mức không nằm trong ordered_values (mặc định NaN)\n",
    "    Output:\n",
    "        np.ndarray: mảng float 1 chiều đã được encode (0..k-1) hoặc unknown_value\n",
    "    \"\"\"\n",
    "    col = np.char.strip(col.astype(str), chars=\" \\\"'\")\n",
    "    encoded = np.full(col.shape[0], unknown_value, dtype=float)\n",
    "\n",
    "    for i, v in enumerate(ordered_values):\n",
    "        encoded[col == v] = float(i)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def encode_binary(col: np.ndarray, mapping: dict, unknown_value: float = np.nan) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Mã hoá một cột categorical theo mapping 2 lớp (hoặc vài lớp nhỏ), ví dụ Gender: F->0, M->1.\n",
    "\n",
    "    Input:\n",
    "        col (np.ndarray): mảng 1 chiều (str/bytes)\n",
    "        mapping (dict): ví dụ {'F': 0.0, 'M': 1.0}\n",
    "        unknown_value (float): giá trị cho các case không có trong mapping\n",
    "    Output:\n",
    "        np.ndarray: mảng float 1 chiều\n",
    "    \"\"\"\n",
    "    col = np.char.strip(col.astype(str), chars=\" \\\"'\")\n",
    "    encoded = np.full(col.shape[0], unknown_value, dtype=float)\n",
    "\n",
    "    for k, v in mapping.items():\n",
    "        encoded[col == k] = float(v)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def build_feature_matrix_for_corr(data: np.ndarray) -> tuple[np.ndarray, list]:\n",
    "    \"\"\"\n",
    "    Tạo ma trận X để tính tương quan Pearson (có encode categorical), và danh sách feature_names tương ứng.\n",
    "\n",
    "    Thành phần:\n",
    "        - Cột 0: Attrition_Flag đã mã hoá 0/1\n",
    "        - Các cột tiếp theo: categorical đã encode (Gender + ordinal cho 4 cột còn lại)\n",
    "        - Các cột số: giữ nguyên (loại CLIENTNUM)\n",
    "\n",
    "    Input:\n",
    "        data (np.ndarray): structured array đã được clean string\n",
    "    Output:\n",
    "        X (np.ndarray): shape (n_samples_valid, n_features), dtype float\n",
    "        feature_names (list): tên cột tương ứng với X\n",
    "\n",
    "    Ghi chú:\n",
    "        - Unknown không nên xem là “mức cao nhất”, nên ở đây ta KHÔNG đưa 'Unknown' vào ordered_values,\n",
    "          để nó rơi vào unknown_value=np.nan rồi lọc dòng trước khi tính corr.\n",
    "    \"\"\"\n",
    "    # target\n",
    "    flag = np.char.strip(data[\"Attrition_Flag\"].astype(str), chars=\" \\\"'\")\n",
    "    y = np.where(flag == \"Attrited Customer\", 1.0, 0.0)\n",
    "\n",
    "    # numeric\n",
    "    numeric_cols = [n for n in data.dtype.names if np.issubdtype(data.dtype[n], np.number)]\n",
    "    if \"CLIENTNUM\" in numeric_cols:\n",
    "        numeric_cols.remove(\"CLIENTNUM\")\n",
    "    X_num = np.column_stack([data[c].astype(float) for c in numeric_cols]) if numeric_cols else np.empty((data.shape[0], 0))\n",
    "\n",
    "    # categorical enc\n",
    "    gender = encode_binary(data[\"Gender\"], {\"F\": 0.0, \"M\": 1.0}, unknown_value=np.nan)\n",
    "\n",
    "    edu = encode_ordinal(\n",
    "        data[\"Education_Level\"],\n",
    "        [\"Uneducated\", \"High School\", \"College\", \"Graduate\", \"Post-Graduate\", \"Doctorate\"],\n",
    "        unknown_value=np.nan\n",
    "    )\n",
    "    income = encode_ordinal(\n",
    "        data[\"Income_Category\"],\n",
    "        [\"Less than $40K\", \"$40K - $60K\", \"$60K - $80K\", \"$80K - $120K\", \"$120K +\"],\n",
    "        unknown_value=np.nan\n",
    "    )\n",
    "    card = encode_ordinal(\n",
    "        data[\"Card_Category\"],\n",
    "        [\"Blue\", \"Silver\", \"Gold\", \"Platinum\"],\n",
    "        unknown_value=np.nan\n",
    "    )\n",
    "    marital = encode_ordinal(\n",
    "        data[\"Marital_Status\"],\n",
    "        [\"Single\", \"Married\", \"Divorced\"],\n",
    "        unknown_value=np.nan\n",
    "    )\n",
    "\n",
    "    X_cat = np.column_stack([gender, edu, income, card, marital])\n",
    "    cat_names = [\"Gender_enc\", \"Education_Level_enc\", \"Income_Category_enc\", \"Card_Category_enc\", \"Marital_Status_enc\"]\n",
    "\n",
    "    X = np.column_stack([y, X_cat, X_num])\n",
    "    feature_names = [\"Attrition_Flag\"] + cat_names + numeric_cols\n",
    "\n",
    "    # lọc các dòng có NaN/inf để corr không bị NaN hàng loạt\n",
    "    valid_rows = np.all(np.isfinite(X), axis=1)\n",
    "    X = X[valid_rows]\n",
    "    return X, feature_names\n",
    "\n",
    "\n",
    "def get_top_k_features_from_corr(X: np.ndarray, feature_names: list, k: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    Lấy top-k feature có |corr| lớn nhất với target (cột Attrition_Flag nằm ở index 0).\n",
    "\n",
    "    Input:\n",
    "        X (np.ndarray): ma trận (n_samples, n_features), cột 0 là target\n",
    "        feature_names (list): danh sách tên feature tương ứng với cột X\n",
    "        k (int): số lượng feature cần lấy\n",
    "    Output:\n",
    "        list: danh sách tên top-k features (không bao gồm target)\n",
    "    \"\"\"\n",
    "    corr = np.corrcoef(X, rowvar=False)\n",
    "    corr_with_target = corr[0, 1:]  # bỏ target\n",
    "    abs_corr = np.abs(corr_with_target)\n",
    "\n",
    "    other_features = np.array(feature_names[1:])\n",
    "    top_idx = np.argsort(abs_corr)[::-1][:k]\n",
    "    return other_features[top_idx].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09fda70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessed_topk_structured(data: np.ndarray, top_features: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Tạo dataset đã tiền xử lý dạng structured array float:\n",
    "    - Attrition_Flag -> 0/1\n",
    "    - Chỉ giữ các cột trong top_features (đã encode nếu là categorical *_enc)\n",
    "    - Lọc các dòng có NaN/inf (do Unknown -> NaN) để dữ liệu sạch khi train.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    # target\n",
    "    flag = np.char.strip(data[\"Attrition_Flag\"].astype(str), chars=\" \\\\\\\"'\")\n",
    "    y = np.where(flag == \"Attrited Customer\", 1.0, 0.0)\n",
    "\n",
    "    # tái tạo đúng encoding như lúc tính corr\n",
    "    gender = encode_binary(data[\"Gender\"], {\"F\": 0.0, \"M\": 1.0}, unknown_value=np.nan)\n",
    "    edu = encode_ordinal(\n",
    "        data[\"Education_Level\"],\n",
    "        [\"Uneducated\", \"High School\", \"College\", \"Graduate\", \"Post-Graduate\", \"Doctorate\"],\n",
    "        unknown_value=np.nan\n",
    "    )\n",
    "    income = encode_ordinal(\n",
    "        data[\"Income_Category\"],\n",
    "        [\"Less than $40K\", \"$40K - $60K\", \"$60K - $80K\", \"$80K - $120K\", \"$120K +\"],\n",
    "        unknown_value=np.nan\n",
    "    )\n",
    "    card = encode_ordinal(\n",
    "        data[\"Card_Category\"],\n",
    "        [\"Blue\", \"Silver\", \"Gold\", \"Platinum\"],\n",
    "        unknown_value=np.nan\n",
    "    )\n",
    "    marital = encode_ordinal(\n",
    "        data[\"Marital_Status\"],\n",
    "        [\"Single\", \"Married\", \"Divorced\"],\n",
    "        unknown_value=np.nan\n",
    "    )\n",
    "\n",
    "    encoded_map = {\n",
    "        \"Gender_enc\": gender,\n",
    "        \"Education_Level_enc\": edu,\n",
    "        \"Income_Category_enc\": income,\n",
    "        \"Card_Category_enc\": card,\n",
    "        \"Marital_Status_enc\": marital,\n",
    "    }\n",
    "\n",
    "    feature_cols = []\n",
    "    final_names = [\"Attrition_Flag\"]\n",
    "\n",
    "    for f in top_features:\n",
    "        if f in encoded_map:\n",
    "            feature_cols.append(encoded_map[f].astype(float))\n",
    "        else:\n",
    "            feature_cols.append(data[f].astype(float))\n",
    "        final_names.append(f)\n",
    "\n",
    "    M = np.column_stack([y] + feature_cols).astype(float)\n",
    "\n",
    "    # lọc dòng không hợp lệ\n",
    "    valid_rows = np.all(np.isfinite(M), axis=1)\n",
    "    M = M[valid_rows]\n",
    "\n",
    "    # build structured array float\n",
    "    dtype = [(name, \"f8\") for name in final_names]\n",
    "    out = np.empty(M.shape[0], dtype=dtype)\n",
    "    for j, name in enumerate(final_names):\n",
    "        out[name] = M[:, j]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def save_float_structured_csv(data: np.ndarray, out_path: str) -> None:\n",
    "    \"\"\"Lưu structured array (toàn float) ra CSV (có header).\"\"\"\n",
    "    if data is None:\n",
    "        print(\"Không có dữ liệu để lưu.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    names = list(data.dtype.names)\n",
    "    arr2d = np.column_stack([data[n] for n in names]).astype(float)\n",
    "    header = \",\".join(names)\n",
    "\n",
    "    np.savetxt(out_path, arr2d, delimiter=\",\", header=header, comments=\"\", fmt=\"%.6g\")\n",
    "    print(\"Đã lưu file:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3211c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_train_valid_test_split(X, y, valid_size=0.1, test_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Chia dữ liệu thành 3 tập: Train, Validation, Test sử dụng Numpy.\n",
    "    \n",
    "    Input:\n",
    "        X: np.ndarray - Ma trận đặc trưng\n",
    "        y: np.ndarray - Biến mục tiêu\n",
    "        valid_size: float - Tỷ lệ tập Validation (Mặc định 0.1)\n",
    "        test_size: float - Tỷ lệ tập Test (Mặc định 0.1)\n",
    "        random_state: int - Seed ngẫu nhiên\n",
    "        \n",
    "    Output:\n",
    "        (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Tính số lượng mẫu\n",
    "    n_valid = int(n_samples * valid_size)\n",
    "    n_test = int(n_samples * test_size)\n",
    "    n_train = n_samples - n_valid - n_test\n",
    "    \n",
    "    # Cắt index\n",
    "    train_idx = indices[:n_train]\n",
    "    val_idx = indices[n_train : n_train + n_valid]\n",
    "    test_idx = indices[n_train + n_valid:]\n",
    "    \n",
    "    return (X[train_idx], y[train_idx]), (X[val_idx], y[val_idx]), (X[test_idx], y[test_idx])\n",
    "\n",
    "def save_modeling_data(filename, X, y, header):\n",
    "    \"\"\"\n",
    "    Lưu dữ liệu modeling xuống file CSV tại folder ../data/modeling/\n",
    "    \"\"\"\n",
    "    # Tạo folder nếu chưa có\n",
    "    MODELING_PATH = '../data/modeling'\n",
    "    if not os.path.exists(MODELING_PATH):\n",
    "        os.makedirs(MODELING_PATH)\n",
    "        \n",
    "    # Reshape y để thành cột dọc (n, 1) nếu cần\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "    data_to_save = np.hstack((X, y))\n",
    "    filepath = os.path.join(MODELING_PATH, filename)\n",
    "    \n",
    "    # Lưu bằng numpy\n",
    "    np.savetxt(filepath, data_to_save, delimiter=\",\", header=header, comments='', fmt='%f')\n",
    "    print(f\"Đã lưu file: {filepath} | Shape: {data_to_save.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a177aa",
   "metadata": {},
   "source": [
    "## Load dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79aca40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã load dữ liệu thành công!\n"
     ]
    }
   ],
   "source": [
    "data_np = load_data_numpy('../data/raw/original_data.csv', clean_strings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e712f6",
   "metadata": {},
   "source": [
    "#  2. Tiền xử lý sơ bộ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4747cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các cột bị loại:\n",
      "- Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\n",
      "- Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\n",
      "Top 10 features (sau encode) theo |corr| với Attrition_Flag:\n",
      "- Total_Trans_Ct: corr = -0.3573\n",
      "- Total_Ct_Chng_Q4_Q1: corr = -0.2805\n",
      "- Total_Revolving_Bal: corr = -0.2654\n",
      "- Contacts_Count_12_mon: corr = 0.1939\n",
      "- Avg_Utilization_Ratio: corr = -0.1840\n",
      "- Total_Trans_Amt: corr = -0.1604\n",
      "- Months_Inactive_12_mon: corr = 0.1527\n",
      "- Total_Relationship_Count: corr = -0.1457\n",
      "- Total_Amt_Chng_Q4_Q1: corr = -0.1327\n",
      "- Gender_enc: corr = -0.0354\n",
      "\n",
      "Dataset sau preprocessing:\n",
      "dtype names: ('Attrition_Flag', 'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Total_Revolving_Bal', 'Contacts_Count_12_mon', 'Avg_Utilization_Ratio', 'Total_Trans_Amt', 'Months_Inactive_12_mon', 'Total_Relationship_Count', 'Total_Amt_Chng_Q4_Q1', 'Gender_enc')\n",
      "shape: (10127,)\n",
      "churn rate (mean y): 0.1606596227905599\n",
      "Đã lưu file: ..\\data\\processed\\preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "# 1) Drop Naive Bayes\n",
    "data_np = drop_naive_bayes_columns(data_np)\n",
    "\n",
    "# 2) Tính corr sau khi encode (để lấy top 10)\n",
    "X_all, feature_names = build_feature_matrix_for_corr(data_np)\n",
    "top10_features = get_top_k_features_from_corr(X_all, feature_names, k=10)\n",
    "\n",
    "print(\"Top 10 features (sau encode) theo |corr| với Attrition_Flag:\")\n",
    "corr = np.corrcoef(X_all, rowvar=False)\n",
    "for f in top10_features:\n",
    "    idx = feature_names.index(f)\n",
    "    print(f\"- {f}: corr = {corr[0, idx]:.4f}\")\n",
    "\n",
    "# 3) Tạo data_np mới: chỉ gồm target + top10 (đã encode) rồi gán lại\n",
    "data_np = make_preprocessed_topk_structured(data_np, top10_features)\n",
    "\n",
    "print(\"\\nDataset sau preprocessing:\")\n",
    "print(\"dtype names:\", data_np.dtype.names)\n",
    "print(\"shape:\", data_np.shape)\n",
    "print(\"churn rate (mean y):\", data_np[\"Attrition_Flag\"].mean())\n",
    "\n",
    "# 4) Lưu ra data/processed/\n",
    "out_csv = os.path.join(\"..\", \"data\", \"processed\", \"preprocessed_data.csv\")\n",
    "save_float_structured_csv(data_np, out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "341797e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã load processed_np!\n",
      "shape: (10127,)\n",
      "columns: ('Attrition_Flag', 'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Total_Revolving_Bal', 'Contacts_Count_12_mon', 'Avg_Utilization_Ratio', 'Total_Trans_Amt', 'Months_Inactive_12_mon', 'Total_Relationship_Count', 'Total_Amt_Chng_Q4_Q1', 'Gender_enc')\n"
     ]
    }
   ],
   "source": [
    "# đọc lại file processed vào biến mới để tiện re run code\n",
    "# để đảm bảo lưu đúng kiểu float các cột số, chúng ta dùng genfromtxt\n",
    "file_processed = '../data/processed/preprocessed_data.csv'\n",
    "\n",
    "processed_np = np.genfromtxt(\n",
    "    file_processed,\n",
    "    delimiter=\",\",\n",
    "    names=True,\n",
    "    dtype=float,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(\"Đã load processed_np!\")\n",
    "print(\"shape:\", processed_np.shape)\n",
    "print(\"columns:\", processed_np.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8de9a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0., 42., 1.625,  777., 3., 0.061, 1144., 1., 5., 1.335, 1.),\n",
       "       (0., 33., 3.714,  864., 2., 0.105, 1291., 1., 6., 1.541, 0.),\n",
       "       (0., 20., 2.333,    0., 0., 0.   , 1887., 1., 4., 2.594, 1.),\n",
       "       (0., 20., 2.333, 2517., 1., 0.76 , 1171., 4., 3., 1.405, 0.),\n",
       "       (0., 28., 2.5  ,    0., 0., 0.   ,  816., 1., 5., 2.175, 1.)],\n",
       "      dtype=[('Attrition_Flag', '<f8'), ('Total_Trans_Ct', '<f8'), ('Total_Ct_Chng_Q4_Q1', '<f8'), ('Total_Revolving_Bal', '<f8'), ('Contacts_Count_12_mon', '<f8'), ('Avg_Utilization_Ratio', '<f8'), ('Total_Trans_Amt', '<f8'), ('Months_Inactive_12_mon', '<f8'), ('Total_Relationship_Count', '<f8'), ('Total_Amt_Chng_Q4_Q1', '<f8'), ('Gender_enc', '<f8')])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_np[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec666c8e",
   "metadata": {},
   "source": [
    "# 3. Tạo giả thuyết"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61caa8ff",
   "metadata": {},
   "source": [
    "Ở bước này chúng ta sẽ đến với việc tạo và thử nghiệm các giả thuyết. Vì lý do hạn chế khả năng và thời gian, em chỉ có thể đưa ra hai giả thuyết. \n",
    "\n",
    "Mô hình sẽ sử dụng là Logistic Regression vì dễ cài đặt bằng Numpy, dễ hiểu (đã được học)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff0680",
   "metadata": {},
   "source": [
    "## 3.1. Giả thuyết 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17917d3",
   "metadata": {},
   "source": [
    "### Mô hình Hoạt động Tuyến tính\n",
    "\n",
    "**Mô tả giả thuyết:**\n",
    "Đây là mô hình cơ sở (Baseline). Giả thuyết cho rằng quyết định rời bỏ là kết quả của phép cộng dồn tuyến tính giữa các chỉ số hoạt động quan trọng nhất. Các biến số tác động độc lập với nhau, không có sự tương tác chéo.\n",
    "\n",
    "Mô hình sẽ sử dụng 3 biến đặc trưng có tương quan mạnh nhất (theo kết quả EDA):\n",
    "1.  `Total_Trans_Ct` (Tổng số lần giao dịch) - Tương quan Âm.\n",
    "2.  `Total_Revolving_Bal` (Tổng dư nợ xoay vòng) - Tương quan Âm.\n",
    "3.  `Contacts_Count_12_mon` (Số lần liên hệ) - Tương quan Dương.\n",
    "\n",
    "**Cơ sở thực tế (Rationale):**\n",
    "* **Tính đơn giản:** Dựa trên tư duy thông thường: Khách hàng càng giao dịch nhiều + càng nợ nhiều + càng ít phàn nàn => Càng gắn bó.\n",
    "* **Đại diện đa chiều:** 3 biến này đại diện cho 3 khía cạnh khác nhau: Hành vi tiêu dùng, Tình trạng tài chính và Sự hài lòng.\n",
    "\n",
    "**Công thức Logistic (Linear):**\n",
    "$$P(Y=1|X) = \\sigma(w_0 + w_1 \\cdot \\text{Trans\\_Ct} + w_2 \\cdot \\text{Rev\\_Bal} + w_3 \\cdot \\text{Contacts})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f591b7",
   "metadata": {},
   "source": [
    "### Chia tập train, valid, test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "465ef938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1 Dataset shape: (10127, 3)\n",
      "Đã lưu file: ../data/modeling\\h1_train.csv | Shape: (8103, 4)\n",
      "Đã lưu file: ../data/modeling\\h1_valid.csv | Shape: (1012, 4)\n",
      "Đã lưu file: ../data/modeling\\h1_test.csv | Shape: (1012, 4)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load dữ liệu\n",
    "data_path = '../data/processed/preprocessed_data.csv'\n",
    "with open(data_path, 'r') as f:\n",
    "    headers = f.readline().strip().split(',')\n",
    "\n",
    "# Map index\n",
    "idx_target = headers.index('Attrition_Flag')\n",
    "idx_ct = headers.index('Total_Trans_Ct')\n",
    "idx_bal = headers.index('Total_Revolving_Bal')\n",
    "idx_contact = headers.index('Contacts_Count_12_mon')\n",
    "\n",
    "data_full = np.genfromtxt(data_path, delimiter=',', skip_header=1)\n",
    "\n",
    "# 2. Chọn Features\n",
    "feat_ct = data_full[:, idx_ct]\n",
    "feat_bal = data_full[:, idx_bal]\n",
    "feat_contact = data_full[:, idx_contact]\n",
    "target = data_full[:, idx_target]\n",
    "\n",
    "# Gom lại thành ma trận X\n",
    "X_h1 = np.column_stack((feat_ct, feat_bal, feat_contact))\n",
    "y_h1 = target\n",
    "\n",
    "print(f\"H1 Dataset shape: {X_h1.shape}\")\n",
    "\n",
    "# 3. Chia tập Train/Valid/Test (80/10/10)\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = numpy_train_valid_test_split(\n",
    "    X_h1, y_h1, valid_size=0.1, test_size=0.1\n",
    ")\n",
    "\n",
    "# 4. Lưu file modeling\n",
    "header_h1 = \"Total_Trans_Ct,Total_Revolving_Bal,Contacts_Count_12_mon,Attrition_Flag\"\n",
    "save_modeling_data('h1_train.csv', X_train, y_train, header_h1)\n",
    "save_modeling_data('h1_valid.csv', X_val, y_val, header_h1)\n",
    "save_modeling_data('h1_test.csv', X_test, y_test, header_h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edff803c",
   "metadata": {},
   "source": [
    "## 3.2. Giả thuyết 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049867be",
   "metadata": {},
   "source": [
    "### Interaction \"Dead Zone\" Model (Mô hình Tương tác \"Vùng Tử Thần\")\n",
    "\n",
    "**Mô tả giả thuyết:**\n",
    "Giả thuyết này cho rằng rủi ro rời bỏ đến từ sự cộng hưởng tiêu cực giữa **Tần suất** và **Giá trị chi tiêu**.\n",
    "\n",
    "Mô hình sử dụng:\n",
    "1.  Các biến gốc: `Total_Trans_Ct`, `Total_Trans_Amt`, `Total_Revolving_Bal`.\n",
    "2.  **Biến tương tác:** `Engagement_Score = Total_Trans_Ct * Total_Trans_Amt`.\n",
    "\n",
    "**Công thức Logistic (Interaction):**\n",
    "$$P(Y=1|X) = \\sigma(w_0 + w_1 \\cdot Ct + w_2 \\cdot Amt + w_3 \\cdot Bal + w_4 \\cdot (Ct \\times Amt))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "610f966b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2 Dataset shape: (10127, 4)\n",
      "Đã lưu file: ../data/modeling\\h2_train.csv | Shape: (8103, 5)\n",
      "Đã lưu file: ../data/modeling\\h2_valid.csv | Shape: (1012, 5)\n",
      "Đã lưu file: ../data/modeling\\h2_test.csv | Shape: (1012, 5)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load dữ liệu (để H2 chạy độc lập, không phụ thuộc H1)\n",
    "data_path = '../data/processed/preprocessed_data.csv'\n",
    "with open(data_path, 'r') as f:\n",
    "    headers = f.readline().strip().split(',')\n",
    "\n",
    "data_full = np.genfromtxt(data_path, delimiter=',', skip_header=1)\n",
    "\n",
    "# 2. Map index\n",
    "idx_target = headers.index('Attrition_Flag')\n",
    "idx_ct = headers.index('Total_Trans_Ct')\n",
    "idx_amt = headers.index('Total_Trans_Amt')\n",
    "idx_bal = headers.index('Total_Revolving_Bal')\n",
    "\n",
    "# 3. Feature Engineering\n",
    "feat_ct = data_full[:, idx_ct]\n",
    "feat_amt = data_full[:, idx_amt]\n",
    "feat_bal = data_full[:, idx_bal]\n",
    "target = data_full[:, idx_target]\n",
    "\n",
    "# Biến tương tác (đúng theo mô tả: Engagement_Score = Ct * Amt)\n",
    "feat_engagement = feat_ct * feat_amt\n",
    "\n",
    "# Gom lại thành ma trận X\n",
    "X_h2 = np.column_stack((feat_ct, feat_amt, feat_bal, feat_engagement))\n",
    "y_h2 = target\n",
    "\n",
    "print(f\"H2 Dataset shape: {X_h2.shape}\")\n",
    "\n",
    "# 4. Chia tập Train/Valid/Test (80/10/10)\n",
    "(X_train_h2, y_train_h2), (X_val_h2, y_val_h2), (X_test_h2, y_test_h2) = numpy_train_valid_test_split(\n",
    "    X_h2, y_h2, valid_size=0.1, test_size=0.1\n",
    ")\n",
    "\n",
    "# 5. Lưu file modeling\n",
    "header_h2 = \"Total_Trans_Ct,Total_Trans_Amt,Total_Revolving_Bal,Engagement_Score,Attrition_Flag\"\n",
    "save_modeling_data('h2_train.csv', X_train_h2, y_train_h2, header_h2)\n",
    "save_modeling_data('h2_valid.csv', X_val_h2, y_val_h2, header_h2)\n",
    "save_modeling_data('h2_test.csv', X_test_h2, y_test_h2, header_h2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
